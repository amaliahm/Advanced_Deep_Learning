{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SARSA_Agent:\n",
    "\n",
    "    def __init__(self, env, alpha, gamma, max_epsilon, min_epsilon, decay_rate):\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        # table with q-values: n_states * n_actions\n",
    "        self.q_table=np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "        # hyper-parameters\n",
    "        self.alpha = alpha                      # learning rate\n",
    "        self.gamma = gamma                      # discount factor\n",
    "        self.max_epsilon = max_epsilon          # Exploration probability at start\n",
    "        self.min_epsilon = min_epsilon          # Minimum exploration probability\n",
    "        self.decay_rate =  decay_rate           # Exponential decay rate for exploration prob\n",
    "\n",
    "\n",
    "\n",
    "    # get action using epsilon greedy polic\n",
    "    def get_action(self, state, epsilon):\n",
    "        # Randomly generate a number between 0 and 1\n",
    "        random_num =random.uniform(0,1)\n",
    "\n",
    "        # if random_num > greater than epsilon --> exploitation\n",
    "        if random_num > epsilon:\n",
    "           # Take the action with the highest value given a state\n",
    "           # np.argmax can be useful here\n",
    "           action = np.argmax(self.q_table[state])\n",
    "\n",
    "        # else --> exploration\n",
    "        else:\n",
    "           action = self.env.action_space.sample()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update_parameters(self, state, action, reward, next_state,epsilon):\n",
    "        \"\"\"\"\"\"\n",
    "        # SARSA-learning formula\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a)  + gamma * Q(s',a') - Q(s,a)]\n",
    "         #--- add code here (4 lines)----------\n",
    "\n",
    "        # update the q_table\n",
    "        self.q_table[state, action] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, max_steps, env, agent):\n",
    "\n",
    "  episode_rewards = []\n",
    "  episode_penalties = []\n",
    "  episode_steps = []\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "\n",
    "    total_rewards_ep = 0\n",
    "    total_penalties_ep=0\n",
    "    total_steps_ep=0\n",
    "\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = agent.min_epsilon + (agent.max_epsilon - agent.min_epsilon)*np.exp(-agent.decay_rate*episode)\n",
    "\n",
    "    # Reset the environment\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    done=False\n",
    "\n",
    "    # repeat\n",
    "    while not done:\n",
    "      # Choose the action At using epsilon greedy policy\n",
    "      action = agent.get_action(state, epsilon)\n",
    "\n",
    "      # Take action At and observe Rt+1 and St+1\n",
    "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "      next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      total_rewards_ep += reward\n",
    "      total_steps_ep +=1\n",
    "      if reward == -10:\n",
    "            total_penalties_ep += 1\n",
    "\n",
    "      # Update Q(s,a):= Q(s,a) + lr [R(s,a)  + gamma * Q(s',a') - Q(s,a)]\n",
    "      agent.update_parameters(state, action, reward, next_state, epsilon)\n",
    "      # If terminated or truncated finish the episode\n",
    "      done=terminated or truncated\n",
    "\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "      # Our next state is the new state\n",
    "      state = next_state\n",
    "\n",
    "      episode_rewards.append(total_rewards_ep)\n",
    "      episode_steps.append(total_steps_ep)\n",
    "      episode_penalties.append(total_penalties_ep)\n",
    "\n",
    "  return  episode_rewards,episode_steps, episode_penalties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 1000  # Total training episodes\n",
    "alpha= 0.7          # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "\n",
    "# Environment parameters\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "eval_seed = []               # The evaluation seed of the environment\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05            # Minimum exploration probability\n",
    "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#env=gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")\n",
    "env=gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "agent=SARSA_Agent(env, alpha, gamma, max_epsilon, min_epsilon, decay_rate)\n",
    "\n",
    "episode_rewards,episode_steps, episode_penalties=train(n_training_episodes, max_steps, env,agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Cumulative Rewards per ride\")\n",
    "pd.Series(episode_rewards).plot(kind='line')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Timesteps to complete ride\")\n",
    "pd.Series(episode_steps).plot(kind='line')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Penalties per ride\")\n",
    "pd.Series(episode_penalties).plot(kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluate the QAgent after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env,agent,  n_eval_episodes):\n",
    "  \"\"\"\n",
    "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "  :param agent: the gent within its  evaluation environment and Qtable\n",
    "  :param max_steps: Maximum number of steps per episode\n",
    "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "   \"\"\"\n",
    "  episode_rewards = []\n",
    "  episode_penalties = []\n",
    "  episode_steps = []\n",
    "  for episode in tqdm(range(n_eval_episodes)):\n",
    "\n",
    "    state, info= env.reset()\n",
    "    step = 0\n",
    "    truncated = False\n",
    "    terminated = False\n",
    "    total_rewards_ep = 0\n",
    "    total_penalties_ep=0\n",
    "    total_steps_ep=0\n",
    "    done= False\n",
    "\n",
    "    while not done:\n",
    "      # Take the action (index) that have the maximum expected future reward given that state\n",
    "      # we use epsilon=0 for exploitation\n",
    "\n",
    "      action = agent.get_action(state,0)\n",
    "      next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      total_rewards_ep += reward\n",
    "      total_steps_ep+=1\n",
    "\n",
    "      if reward == -10:\n",
    "            total_penalties_ep += 1\n",
    "\n",
    "      done=terminated or truncated\n",
    "\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "      state = next_state\n",
    "\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "    episode_steps.append(total_steps_ep)\n",
    "    episode_penalties.append(total_penalties_ep)\n",
    "\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward, episode_rewards,episode_steps, episode_penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_reward, std_reward,episode_rewards,episode_steps, episode_penalties=evaluate_agent(env,agent, 1000)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Cumulative Rewards per ride\")\n",
    "pd.Series(episode_rewards).plot(kind='line')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Timesteps to complete ride\")\n",
    "pd.Series(episode_steps).plot(kind='line')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Penalties per ride\")\n",
    "pd.Series(episode_penalties).plot(kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Record a simulation as a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def record_video(env, agent, out_directory, fps=1):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param agent:  agent within its Qtable\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  images = []\n",
    "  terminated = False\n",
    "  truncated = False\n",
    "  state, info = env.reset(seed=random.randint(0,500))\n",
    "  img = env.render()\n",
    "  images.append(img)\n",
    "  while not terminated or truncated:\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action = np.argmax(agent.q_table[state][:])\n",
    "    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "# generate the video\n",
    "video_path = \"./replay.mp4\"\n",
    "record_video(env, agent, video_path, 1)\n",
    "\n",
    "# Show video\n",
    "mp4 = open(video_path,'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"<video width=400 controls>      <source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
